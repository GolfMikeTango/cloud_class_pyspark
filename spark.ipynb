{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/01 13:26:47 WARN Utils: Your hostname, gadmin-virtual-machine resolves to a loopback address: 127.0.1.1; using 192.168.3.155 instead (on interface ens160)\n",
      "23/04/01 13:26:47 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/01 13:26:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/01 13:26:56 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gadmin/git/cloud_class/cloud_class_pyspark/spark-3.3.2-bin-hadoop3/python/pyspark/sql/pandas/conversion.py:248: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead\n",
      "  series = series.astype(t, copy=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- STATION: long (nullable = true)\n",
      " |-- DATE: timestamp (nullable = true)\n",
      " |-- LATITUDE: double (nullable = true)\n",
      " |-- LONGITUDE: double (nullable = true)\n",
      " |-- ELEVATION: double (nullable = true)\n",
      " |-- NAME: string (nullable = true)\n",
      " |-- TEMP: double (nullable = true)\n",
      " |-- TEMP_ATTRIBUTES: double (nullable = true)\n",
      " |-- DEWP: double (nullable = true)\n",
      " |-- DEWP_ATTRIBUTES: double (nullable = true)\n",
      " |-- SLP: double (nullable = true)\n",
      " |-- SLP_ATTRIBUTES: double (nullable = true)\n",
      " |-- STP: double (nullable = true)\n",
      " |-- STP_ATTRIBUTES: double (nullable = true)\n",
      " |-- VISIB: double (nullable = true)\n",
      " |-- VISIB_ATTRIBUTES: double (nullable = true)\n",
      " |-- WDSP: double (nullable = true)\n",
      " |-- WDSP_ATTRIBUTES: double (nullable = true)\n",
      " |-- MXSPD: double (nullable = true)\n",
      " |-- GUST: double (nullable = true)\n",
      " |-- MAX: double (nullable = true)\n",
      " |-- MAX_ATTRIBUTES: string (nullable = true)\n",
      " |-- MIN: double (nullable = true)\n",
      " |-- MIN_ATTRIBUTES: string (nullable = true)\n",
      " |-- PRCP: double (nullable = true)\n",
      " |-- PRCP_ATTRIBUTES: string (nullable = true)\n",
      " |-- SNDP: double (nullable = true)\n",
      " |-- FRSHTT: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gadmin/git/cloud_class/cloud_class_pyspark/spark-3.3.2-bin-hadoop3/python/pyspark/sql/dataframe.py:229: FutureWarning: Deprecated in 2.0, use createOrReplaceTempView instead.\n",
      "  warnings.warn(\"Deprecated in 2.0, use createOrReplaceTempView instead.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import findspark\n",
    "from pprint import pprint\n",
    "\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession, Window\n",
    "import pyspark.sql.functions as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "weather_paths = [str(p.resolve()) for p in Path(\"weather_data\").glob(\"*/*\")]\n",
    "\n",
    "rows = spark.read.csv(weather_paths, header=True, inferSchema=True)\n",
    "pd_df = rows.toPandas()\n",
    "rows.printSchema()\n",
    "rows.registerTempTable(\"weather\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "years = spark.sql(\"select distinct year(date) as year from weather\").orderBy('year').collect()\n",
    "df_dict = {}\n",
    "for y in years:\n",
    "    df_dict[y.year] = rows.filter(F.year(\"date\") == y.year)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Find the hottest day (column MAX) for each year, and provide the corresponding station code,\n",
    "station name and the date (columns STATION, NAME, DATE).   > There should be 13 results.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 63:>                                                         (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------+-----------+--------------------+\n",
      "|               date|   max|    station|                name|\n",
      "+-------------------+------+-----------+--------------------+\n",
      "|2010-08-15 00:00:00|  74.8|99407099999|DESTRUCTION IS. W...|\n",
      "|2011-07-09 00:00:00|  87.8| 1046099999|       SORKJOSEN, NO|\n",
      "|2012-07-05 00:00:00|  72.0| 1023099999|       BARDUFOSS, NO|\n",
      "|2013-01-03 00:00:00|9999.9| 1001499999|      SORSTOKKEN, NO|\n",
      "|2014-07-10 00:00:00|  89.6| 1023099999|       BARDUFOSS, NO|\n",
      "|2015-07-30 00:00:00|  71.6| 1025099999|          TROMSO, NO|\n",
      "|2016-03-10 00:00:00|9999.9| 1023199999|         DRAUGEN, NO|\n",
      "|2017-06-09 00:00:00|  78.6| 1023099999|       BARDUFOSS, NO|\n",
      "|2018-07-29 00:00:00|  84.2| 1025099999|          TROMSO, NO|\n",
      "|2019-07-21 00:00:00|  78.8| 1023099999|       BARDUFOSS, NO|\n",
      "|2020-06-22 00:00:00|  79.9| 1023099999|       BARDUFOSS, NO|\n",
      "|2021-07-05 00:00:00|  88.3| 1065099999|        KARASJOK, NO|\n",
      "|2022-07-01 00:00:00|  85.5| 2095099999|          PAJALA, SW|\n",
      "+-------------------+------+-----------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "max_temp = []\n",
    "for key, df_weather in df_dict.items():\n",
    "        max_filter = df_weather.select(\"date\", \"max\", \"station\", \"name\").orderBy(F.desc(\"max\"))\n",
    "        max_temp.append(max_filter.collect()[0])\n",
    "max_df = spark.createDataFrame(max_temp)\n",
    "max_df.select(\"date\", \"max\", \"station\", \"name\").orderBy(F.asc(\"date\")).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Find the coldest day (column MIN) for the month of January across all years (2010 - 2022) ,\n",
    "and provide the corresponding station code, station name and the date (columns STATION,\n",
    "NAME, DATE). > There should be 1 result\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+----------+--------------------+\n",
      "|               date|  min|   station|                name|\n",
      "+-------------------+-----+----------+--------------------+\n",
      "|2017-01-05 00:00:00|-28.3|1023099999|       BARDUFOSS, NO|\n",
      "|2022-01-08 00:00:00|-28.1|2095099999|          PAJALA, SW|\n",
      "|2021-01-13 00:00:00|-27.2|1065099999|        KARASJOK, NO|\n",
      "|2011-01-30 00:00:00|-23.8|1008099999|        LONGYEAR, SV|\n",
      "|2019-01-31 00:00:00|-22.0|1023099999|       BARDUFOSS, NO|\n",
      "|2020-01-27 00:00:00|-19.3|1023099999|       BARDUFOSS, NO|\n",
      "|2014-01-12 00:00:00|-18.9|1023099999|       BARDUFOSS, NO|\n",
      "|2015-01-30 00:00:00| -8.7|1008099999|        LONGYEAR, SV|\n",
      "|2012-01-30 00:00:00| -8.1|1023099999|       BARDUFOSS, NO|\n",
      "|2018-01-03 00:00:00| -2.2|1008099999|        LONGYEAR, SV|\n",
      "|2013-01-27 00:00:00| -1.3|1008099999|        LONGYEAR, SV|\n",
      "|2010-01-29 00:00:00| -1.1|1052099999|HAMMERFEST AIRPOR...|\n",
      "|2016-01-21 00:00:00| 10.4|1008099999|        LONGYEAR, SV|\n",
      "+-------------------+-----+----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "min_temp = []\n",
    "for key, df_weather in df_dict.items():\n",
    "        min_filter = df_weather.select(\"date\", \"min\", \"station\", \"name\").filter(F.month(\"date\") == 1).orderBy(F.asc(\"min\"))\n",
    "        min_temp.append(min_filter.collect()[0])\n",
    "min_df = spark.createDataFrame(min_temp)\n",
    "min_df.select(\"date\", \"min\", \"station\", \"name\").orderBy(F.asc(\"min\")).show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Maximum and Minimum precipitation (column PRCP ) for the year 2015, and provide the\n",
    "corresponding station code, station name and the date (columns STATION, NAME, DATE). > There should be 2 results.  Any max or min would do.  Just choose 1 or each.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+----------+------------+\n",
      "|               date| prcp|   station|        name|\n",
      "+-------------------+-----+----------+------------+\n",
      "|2015-01-01 00:00:00|  0.0|1008099999|LONGYEAR, SV|\n",
      "|2015-11-18 00:00:00|99.99|1008099999|LONGYEAR, SV|\n",
      "+-------------------+-----+----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "max_min_max_filter = df_dict[2015].select(\"date\", \"prcp\", \"station\", \"name\").orderBy(F.asc(\"prcp\")).collect()[0]\n",
    "max_min_min_filter = df_dict[2015].select(\"date\", \"prcp\", \"station\", \"name\").orderBy(F.desc(\"prcp\")).collect()[0]\n",
    "max_min_df = spark.createDataFrame([max_min_max_filter, max_min_min_filter])\n",
    "max_min_df.select(\"*\").show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Count percentage missing values for wind gust (column GUST) for the year 2019. > There should be 1 result\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of missing gusts is: 82.88%\n"
     ]
    }
   ],
   "source": [
    "gust_total = df_dict[2019].select(\"date\", \"gust\", \"station\", \"name\").orderBy(F.desc(\"gust\")).count()\n",
    "gust_missing = df_dict[2019].select(\"date\", \"gust\", \"station\", \"name\").filter(F.col(\"gust\") == \"999.9\").count()\n",
    "print(\"Percentage of missing gusts is: \" + format(gust_missing/gust_total, \".2%\"))\n",
    "#max_min_df.select(\"*\").show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Find the mean, median, mode and standard deviation of the Temperature (column TEMP) for\n",
    "each month for the year 2020. > There should be 12 results, one for each month with 4 values for each result(row).\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+------+------+----+\n",
      "|Month|  MEAN|STDDEV|MEDIAN|MODE|\n",
      "+-----+------+------+------+----+\n",
      "|    1|15.897|12.805| 15.25| 5.7|\n",
      "+-----+------+------+------+----+\n",
      "\n",
      "+-----+------+------+------+----+\n",
      "|Month|  MEAN|STDDEV|MEDIAN|MODE|\n",
      "+-----+------+------+------+----+\n",
      "|    2|13.359|13.092|  15.4| 8.7|\n",
      "+-----+------+------+------+----+\n",
      "\n",
      "+-----+------+------+------+----+\n",
      "|Month|  MEAN|STDDEV|MEDIAN|MODE|\n",
      "+-----+------+------+------+----+\n",
      "|    3|14.653|15.785|  18.6|18.6|\n",
      "+-----+------+------+------+----+\n",
      "\n",
      "+-----+-----+------+------+----+\n",
      "|Month| MEAN|STDDEV|MEDIAN|MODE|\n",
      "+-----+-----+------+------+----+\n",
      "|    4|23.33|13.022|  27.3|34.1|\n",
      "+-----+-----+------+------+----+\n",
      "\n",
      "+-----+------+------+------+----+\n",
      "|Month|  MEAN|STDDEV|MEDIAN|MODE|\n",
      "+-----+------+------+------+----+\n",
      "|    5|36.219| 8.077| 36.05|37.0|\n",
      "+-----+------+------+------+----+\n",
      "\n",
      "+-----+-----+------+------+----+\n",
      "|Month| MEAN|STDDEV|MEDIAN|MODE|\n",
      "+-----+-----+------+------+----+\n",
      "|    6|47.43| 8.877|  46.1|37.8|\n",
      "+-----+-----+------+------+----+\n",
      "\n",
      "+-----+------+------+------+----+\n",
      "|Month|  MEAN|STDDEV|MEDIAN|MODE|\n",
      "+-----+------+------+------+----+\n",
      "|    7|52.887| 6.664| 51.55|49.3|\n",
      "+-----+------+------+------+----+\n",
      "\n",
      "+-----+------+------+------+----+\n",
      "|Month|  MEAN|STDDEV|MEDIAN|MODE|\n",
      "+-----+------+------+------+----+\n",
      "|    8|49.287| 6.549| 48.85|44.7|\n",
      "+-----+------+------+------+----+\n",
      "\n",
      "+-----+------+------+------+----+\n",
      "|Month|  MEAN|STDDEV|MEDIAN|MODE|\n",
      "+-----+------+------+------+----+\n",
      "|    9|41.845| 5.888| 42.55|43.1|\n",
      "+-----+------+------+------+----+\n",
      "\n",
      "+-----+------+------+------+----+\n",
      "|Month|  MEAN|STDDEV|MEDIAN|MODE|\n",
      "+-----+------+------+------+----+\n",
      "|   10|31.529| 9.609|  30.9|24.0|\n",
      "+-----+------+------+------+----+\n",
      "\n",
      "+-----+------+------+------+----+\n",
      "|Month|  MEAN|STDDEV|MEDIAN|MODE|\n",
      "+-----+------+------+------+----+\n",
      "|   11|29.247| 8.143|  29.9|28.1|\n",
      "+-----+------+------+------+----+\n",
      "\n",
      "+-----+------+------+------+----+\n",
      "|Month|  MEAN|STDDEV|MEDIAN|MODE|\n",
      "+-----+------+------+------+----+\n",
      "|   12|19.955| 8.854| 20.35|10.2|\n",
      "+-----+------+------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import FloatType\n",
    "import statistics as st\n",
    "\n",
    "def find_median(val_list):\n",
    "    try:\n",
    "        median = np.median(val_list)\n",
    "        return round(float(median), 2)\n",
    "    except Exception:\n",
    "        return None\n",
    "    \n",
    "def find_mode(val_list):\n",
    "    try:\n",
    "        mode = st.mode(val_list)\n",
    "        return round(float(mode), 2)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "df_dict[2020].registerTempTable(\"2020_weather\")\n",
    "months = spark.sql(\"select distinct month(date) as month from 2020_weather\").orderBy('month').collect()\n",
    "df_dict_2020 = {}\n",
    "for m in months:\n",
    "    df_dict_2020[m.month] = df_dict[2020].filter(F.month(\"date\") == m.month)\n",
    "df_dict_2020\n",
    "\n",
    "median_finder = F.udf(find_median, FloatType())\n",
    "mode_finder = F.udf(find_mode, FloatType())\n",
    "\n",
    "for month, df_data in df_dict_2020.items():\n",
    "    c = df_data.groupBy(F.month(\"date\").alias(\"Month\")).agg(F.collect_list(\"TEMP\").alias(\"TEMP\")\n",
    "                                                            , F.mean(\"TEMP\").alias(\"MEAN\")\n",
    "                                                            , F.stddev(\"TEMP\").alias(\"STDDEV\"))\n",
    "    maths = c.collect()\n",
    "    d = c.withColumn(\"MEAN\", F.round(\"MEAN\", 3)).withColumn(\"MEDIAN\", median_finder(\"TEMP\")).withColumn(\"MODE\", mode_finder(\"TEMP\")).withColumn(\"STDDEV\", F.round(\"STDDEV\", 3)).drop(\"TEMP\")\n",
    "    d.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7d0a192b8570832358513bdfdda752cc3ac66b1c77d9440a9b0610e986c97dc4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
