{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import findspark\n",
    "from pprint import pprint\n",
    "\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession, Window\n",
    "import pyspark.sql.functions as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "weather_paths = [str(p.resolve()) for p in Path(\"weather_data\").glob(\"*/*\")]\n",
    "\n",
    "rows = spark.read.csv(weather_paths, header=True, inferSchema=True)\n",
    "pd_df = rows.toPandas()\n",
    "rows.registerTempTable(\"weather\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "years = spark.sql(\"select distinct year(date) as year from weather\").orderBy('year').collect()\n",
    "df_dict = {}\n",
    "for y in years:\n",
    "    df_dict[y.year] = rows.filter(F.year(\"date\") == y.year)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1 \n",
    "* Find the hottest day (column MAX) for each year, and provide the corresponding station code, station name and the date (columns STATION, NAME, DATE).   > There should be 13 results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_temp = []\n",
    "for key, df_weather in df_dict.items():\n",
    "        max_filter = df_weather.select(\"date\", \"max\", \"station\", \"name\").orderBy(F.desc(\"max\"))\n",
    "        max_temp.append(max_filter.collect()[0])\n",
    "max_df = spark.createDataFrame(max_temp)\n",
    "max_df.select(F.year(\"date\"), \"max\", \"station\", \"name\").orderBy(F.asc(\"date\")).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2:\n",
    "\n",
    "* Find the coldest day (column MIN) for the month of January across all years (2010 - 2022) , and provide the corresponding station code, station name and the date (columns STATION, NAME, DATE). > There should be 1 result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "min_temp = []\n",
    "for key, df_weather in df_dict.items():\n",
    "        min_filter = df_weather.select(\"date\", \"min\", \"station\", \"name\").filter(F.month(\"date\") == 1).orderBy(F.asc(\"min\"))\n",
    "        min_temp.append(min_filter.collect()[0])\n",
    "min_df = spark.createDataFrame(min_temp)\n",
    "min_df.select(\"date\", \"min\", \"station\", \"name\").orderBy(F.asc(\"min\")).show(1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3\n",
    "* Maximum and Minimum precipitation (column PRCP ) for the year 2015, and provide the corresponding station code, station name and the date (columns STATION, NAME, DATE). > There should be 2 results.  Any max or min would do.  Just choose 1 or each.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_min_max_filter = df_dict[2015].select(\"date\", \"prcp\", \"station\", \"name\").orderBy(F.asc(\"prcp\")).collect()[0]\n",
    "max_min_min_filter = df_dict[2015].select(\"date\", \"prcp\", \"station\", \"name\").orderBy(F.desc(\"prcp\")).collect()[0]\n",
    "max_min_df = spark.createDataFrame([max_min_max_filter, max_min_min_filter])\n",
    "max_min_df.select(\"*\").show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4\n",
    "* Count percentage missing values for wind gust (column GUST) for the year 2019. > There should be 1 result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gust_total = df_dict[2019].select(\"date\", \"gust\", \"station\", \"name\").orderBy(F.desc(\"gust\")).count()\n",
    "gust_missing = df_dict[2019].select(\"date\", \"gust\", \"station\", \"name\").filter(F.col(\"gust\") == \"999.9\").count()\n",
    "print(\"Percentage of missing gusts is: \" + format(gust_missing/gust_total, \".2%\"))\n",
    "#max_min_df.select(\"*\").show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5\n",
    "* Find the mean, median, mode and standard deviation of the Temperature (column TEMP) for each month for the year 2020. > There should be 12 results, one for each month with 4 values for each result(row)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import FloatType\n",
    "import statistics as st\n",
    "\n",
    "def find_median(val_list):\n",
    "    try:\n",
    "        median = np.median(val_list)\n",
    "        return round(float(median), 2)\n",
    "    except Exception:\n",
    "        return None\n",
    "    \n",
    "def find_mode(val_list):\n",
    "    try:\n",
    "        mode = st.mode(val_list)\n",
    "        return round(float(mode), 2)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "df_dict[2020].registerTempTable(\"2020_weather\")\n",
    "months = spark.sql(\"select distinct month(date) as month from 2020_weather\").orderBy('month').collect()\n",
    "df_dict_2020 = {}\n",
    "for m in months:\n",
    "    df_dict_2020[m.month] = df_dict[2020].filter(F.month(\"date\") == m.month)\n",
    "df_dict_2020\n",
    "\n",
    "median_finder = F.udf(find_median, FloatType())\n",
    "mode_finder = F.udf(find_mode, FloatType())\n",
    "\n",
    "for month, df_data in df_dict_2020.items():\n",
    "    c = df_data.groupBy(F.month(\"date\").alias(\"Month\")).agg(F.collect_list(\"TEMP\").alias(\"TEMP\")\n",
    "                                                            , F.mean(\"TEMP\").alias(\"MEAN\")\n",
    "                                                            , F.stddev(\"TEMP\").alias(\"STDDEV\"))\n",
    "    maths = c.collect()\n",
    "    d = c.withColumn(\"MEAN\", F.round(\"MEAN\", 3)).withColumn(\"MEDIAN\", median_finder(\"TEMP\")).withColumn(\"MODE\", mode_finder(\"TEMP\")).withColumn(\"STDDEV\", F.round(\"STDDEV\", 3)).drop(\"TEMP\")\n",
    "    d.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7d0a192b8570832358513bdfdda752cc3ac66b1c77d9440a9b0610e986c97dc4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
